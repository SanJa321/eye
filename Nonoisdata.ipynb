{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45ffde0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Noisy Image Count Per Class:\n",
      "  CNV: 1188/51390 images flagged as noisy (2.31%)\n",
      "  DME: 9363/51390 images flagged as noisy (18.22%)\n",
      "  DRUSEN: 9864/51390 images flagged as noisy (19.19%)\n",
      "  NORMAL: 129/51390 images flagged as noisy (0.25%)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "def is_noisy(image_path, blur_thresh=100.0, var_thresh=10.0, entropy_thresh=3.0):\n",
    "    try:\n",
    "        img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if img is None:\n",
    "            return True  # unreadable or empty image\n",
    "\n",
    "        # 1. Blur detection using Laplacian variance\n",
    "        lap_var = cv2.Laplacian(img, cv2.CV_64F).var()\n",
    "        if lap_var < blur_thresh:\n",
    "            return True\n",
    "\n",
    "        # 2. Pixel variance (flat images)\n",
    "        pixel_var = np.var(img)\n",
    "        if pixel_var < var_thresh:\n",
    "            return True\n",
    "\n",
    "        # 3. Entropy (information content)\n",
    "        histogram = cv2.calcHist([img], [0], None, [256], [0, 256]).ravel()\n",
    "        histogram /= histogram.sum()  # normalize\n",
    "        histogram = histogram[histogram > 0]  # remove zeros to avoid log(0)\n",
    "        entropy = -np.sum(histogram * np.log2(histogram))\n",
    "        if entropy < entropy_thresh:\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "    except:\n",
    "        return True  # treat unreadable as noisy\n",
    "\n",
    "def count_noisy_images(folder_path):\n",
    "    noisy_counts = defaultdict(int)\n",
    "    total_counts = defaultdict(int)\n",
    "\n",
    "    for class_dir in os.listdir(folder_path):\n",
    "        class_path = os.path.join(folder_path, class_dir)\n",
    "        if not os.path.isdir(class_path):\n",
    "            continue\n",
    "\n",
    "        for fname in os.listdir(class_path):\n",
    "            fpath = os.path.join(class_path, fname)\n",
    "            total_counts[class_dir] += 1\n",
    "\n",
    "            if is_noisy(fpath):\n",
    "                noisy_counts[class_dir] += 1\n",
    "\n",
    "    return noisy_counts, total_counts\n",
    "\n",
    "# Set your dataset path\n",
    "dataset_path = \"augmented_balanced_data\"\n",
    "noisy_counts, total_counts = count_noisy_images(dataset_path)\n",
    "\n",
    "# Print the summary\n",
    "print(\"\\n📊 Noisy Image Count Per Class:\")\n",
    "for cls in total_counts:\n",
    "    noisy = noisy_counts[cls]\n",
    "    total = total_counts[cls]\n",
    "    print(f\"  {cls}: {noisy}/{total} images flagged as noisy ({(noisy/total)*100:.2f}%)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecfc169b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\anaconda3\\envs\\eyediseaseclassification_env\\lib\\site-packages\\albumentations\\__init__.py:28: UserWarning: A new version of Albumentations is available: '2.0.7' (you have '2.0.6'). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n",
      "c:\\Users\\DELL\\anaconda3\\envs\\eyediseaseclassification_env\\lib\\site-packages\\albumentations\\core\\validation.py:111: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n",
      "  original_init(self, **validated_kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔁 Performing Augmentation to Balance Classes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [03:49<00:00, 57.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Dataset cleaned and balanced with augmentation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import albumentations as A\n",
    "\n",
    "# --- Noisy Image Detection Function ---\n",
    "def is_noisy(image_path, blur_thresh=100.0, var_thresh=10.0, entropy_thresh=3.0):\n",
    "    try:\n",
    "        img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if img is None:\n",
    "            return True\n",
    "\n",
    "        lap_var = cv2.Laplacian(img, cv2.CV_64F).var()\n",
    "        if lap_var < blur_thresh:\n",
    "            return True\n",
    "\n",
    "        pixel_var = np.var(img)\n",
    "        if pixel_var < var_thresh:\n",
    "            return True\n",
    "\n",
    "        histogram = cv2.calcHist([img], [0], None, [256], [0, 256]).ravel()\n",
    "        histogram /= histogram.sum()\n",
    "        histogram = histogram[histogram > 0]\n",
    "        entropy = -np.sum(histogram * np.log2(histogram))\n",
    "        if entropy < entropy_thresh:\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "    except:\n",
    "        return True\n",
    "\n",
    "# --- Augmentation Setup (NO ZOOM) ---\n",
    "transform = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.5),\n",
    "    A.Rotate(limit=15, p=0.5),\n",
    "    A.GaussianBlur(p=0.3),\n",
    "    A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=0, p=0.5)\n",
    "])\n",
    "\n",
    "# --- Clean and Augment Dataset ---\n",
    "def clean_and_augment_data(dataset_path, output_path):\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    class_counts = defaultdict(int)\n",
    "    image_paths = defaultdict(list)\n",
    "\n",
    "    # 1. Remove noisy images and copy clean ones\n",
    "    for class_name in os.listdir(dataset_path):\n",
    "        class_path = os.path.join(dataset_path, class_name)\n",
    "        if not os.path.isdir(class_path):\n",
    "            continue\n",
    "\n",
    "        clean_class_path = os.path.join(output_path, class_name)\n",
    "        os.makedirs(clean_class_path, exist_ok=True)\n",
    "\n",
    "        for fname in os.listdir(class_path):\n",
    "            img_path = os.path.join(class_path, fname)\n",
    "            if not is_noisy(img_path):\n",
    "                dest_path = os.path.join(clean_class_path, fname)\n",
    "                shutil.copy(img_path, dest_path)\n",
    "                class_counts[class_name] += 1\n",
    "                image_paths[class_name].append(dest_path)\n",
    "\n",
    "    # 2. Determine max count\n",
    "    max_count = max(class_counts.values())\n",
    "\n",
    "    # 3. Perform augmentation to balance all classes\n",
    "    print(\"\\n🔁 Performing Augmentation to Balance Classes...\")\n",
    "    for class_name, paths in tqdm(image_paths.items()):\n",
    "        current_count = class_counts[class_name]\n",
    "        class_dir = os.path.join(output_path, class_name)\n",
    "        img_idx = 0\n",
    "        while current_count < max_count:\n",
    "            img_path = paths[img_idx % len(paths)]\n",
    "            image = cv2.imread(img_path)\n",
    "            if image is None:\n",
    "                img_idx += 1\n",
    "                continue\n",
    "\n",
    "            aug = transform(image=image)\n",
    "            aug_img = aug['image']\n",
    "            new_name = f\"aug_{current_count}_{os.path.basename(img_path)}\"\n",
    "            cv2.imwrite(os.path.join(class_dir, new_name), aug_img)\n",
    "            current_count += 1\n",
    "            img_idx += 1\n",
    "\n",
    "    print(\"\\n✅ Dataset cleaned and balanced with augmentation.\")\n",
    "\n",
    "# --- Run ---\n",
    "dataset_path = \"augmented_balanced_data\"\n",
    "output_path = \"clean_balanced_data\"\n",
    "clean_and_augment_data(dataset_path, output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57c20eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Noisy Image Count Per Class:\n",
      "  CNV: 401/51261 images flagged as noisy (0.78%)\n",
      "  DME: 3600/51261 images flagged as noisy (7.02%)\n",
      "  DRUSEN: 3733/51261 images flagged as noisy (7.28%)\n",
      "  NORMAL: 0/51261 images flagged as noisy (0.00%)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "def is_noisy(image_path, blur_thresh=100.0, var_thresh=10.0, entropy_thresh=3.0):\n",
    "    try:\n",
    "        img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if img is None:\n",
    "            return True  # unreadable or empty image\n",
    "\n",
    "        # 1. Blur detection using Laplacian variance\n",
    "        lap_var = cv2.Laplacian(img, cv2.CV_64F).var()\n",
    "        if lap_var < blur_thresh:\n",
    "            return True\n",
    "\n",
    "        # 2. Pixel variance (flat images)\n",
    "        pixel_var = np.var(img)\n",
    "        if pixel_var < var_thresh:\n",
    "            return True\n",
    "\n",
    "        # 3. Entropy (information content)\n",
    "        histogram = cv2.calcHist([img], [0], None, [256], [0, 256]).ravel()\n",
    "        histogram /= histogram.sum()  # normalize\n",
    "        histogram = histogram[histogram > 0]  # remove zeros to avoid log(0)\n",
    "        entropy = -np.sum(histogram * np.log2(histogram))\n",
    "        if entropy < entropy_thresh:\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "    except:\n",
    "        return True  # treat unreadable as noisy\n",
    "\n",
    "def count_noisy_images(folder_path):\n",
    "    noisy_counts = defaultdict(int)\n",
    "    total_counts = defaultdict(int)\n",
    "\n",
    "    for class_dir in os.listdir(folder_path):\n",
    "        class_path = os.path.join(folder_path, class_dir)\n",
    "        if not os.path.isdir(class_path):\n",
    "            continue\n",
    "\n",
    "        for fname in os.listdir(class_path):\n",
    "            fpath = os.path.join(class_path, fname)\n",
    "            total_counts[class_dir] += 1\n",
    "\n",
    "            if is_noisy(fpath):\n",
    "                noisy_counts[class_dir] += 1\n",
    "\n",
    "    return noisy_counts, total_counts\n",
    "\n",
    "# Set your dataset path\n",
    "dataset_path = \"clean_balanced_data\"\n",
    "noisy_counts, total_counts = count_noisy_images(dataset_path)\n",
    "\n",
    "# Print the summary\n",
    "print(\"\\n📊 Noisy Image Count Per Class:\")\n",
    "for cls in total_counts:\n",
    "    noisy = noisy_counts[cls]\n",
    "    total = total_counts[cls]\n",
    "    print(f\"  {cls}: {noisy}/{total} images flagged as noisy ({(noisy/total)*100:.2f}%)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e46c9b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Scanning and filtering noisy images...\n",
      "  CNV: 50860 clean images\n",
      "  DME: 47661 clean images\n",
      "  DRUSEN: 47528 clean images\n",
      "  NORMAL: 51261 clean images\n",
      "\n",
      "🎯 Target image count per class (based on smallest class): 47528\n",
      "\n",
      "📦 Copying balanced dataset...\n",
      "  CNV: 47528 images copied\n",
      "  DME: 47528 images copied\n",
      "  DRUSEN: 47528 images copied\n",
      "  NORMAL: 47528 images copied\n",
      "\n",
      "✅ Done! Noisy images removed and classes balanced.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import shutil\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "def is_noisy(image_path, blur_thresh=100.0, var_thresh=10.0, entropy_thresh=3.0):\n",
    "    try:\n",
    "        img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if img is None:\n",
    "            return True  # unreadable or empty image\n",
    "\n",
    "        lap_var = cv2.Laplacian(img, cv2.CV_64F).var()\n",
    "        if lap_var < blur_thresh:\n",
    "            return True\n",
    "\n",
    "        pixel_var = np.var(img)\n",
    "        if pixel_var < var_thresh:\n",
    "            return True\n",
    "\n",
    "        histogram = cv2.calcHist([img], [0], None, [256], [0, 256]).ravel()\n",
    "        histogram /= histogram.sum()\n",
    "        histogram = histogram[histogram > 0]\n",
    "        entropy = -np.sum(histogram * np.log2(histogram))\n",
    "        if entropy < entropy_thresh:\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "    except:\n",
    "        return True\n",
    "\n",
    "def remove_noisy_and_balance(input_dir, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    class_images = defaultdict(list)\n",
    "\n",
    "    print(\"🔍 Scanning and filtering noisy images...\")\n",
    "    for class_name in os.listdir(input_dir):\n",
    "        class_path = os.path.join(input_dir, class_name)\n",
    "        if not os.path.isdir(class_path):\n",
    "            continue\n",
    "\n",
    "        clean_images = []\n",
    "        for img_name in os.listdir(class_path):\n",
    "            img_path = os.path.join(class_path, img_name)\n",
    "            if not is_noisy(img_path):\n",
    "                clean_images.append(img_path)\n",
    "\n",
    "        class_images[class_name] = clean_images\n",
    "        print(f\"  {class_name}: {len(clean_images)} clean images\")\n",
    "\n",
    "    # Find the minimum count\n",
    "    min_count = min(len(imgs) for imgs in class_images.values())\n",
    "    print(f\"\\n🎯 Target image count per class (based on smallest class): {min_count}\\n\")\n",
    "\n",
    "    print(\"📦 Copying balanced dataset...\")\n",
    "    for class_name, images in class_images.items():\n",
    "        selected_images = random.sample(images, min_count)\n",
    "        dest_class_dir = os.path.join(output_dir, class_name)\n",
    "        os.makedirs(dest_class_dir, exist_ok=True)\n",
    "\n",
    "        for src_path in selected_images:\n",
    "            dst_path = os.path.join(dest_class_dir, os.path.basename(src_path))\n",
    "            shutil.copy(src_path, dst_path)\n",
    "\n",
    "        print(f\"  {class_name}: {len(selected_images)} images copied\")\n",
    "\n",
    "    print(\"\\n✅ Done! Noisy images removed and classes balanced.\")\n",
    "\n",
    "# === Usage ===\n",
    "input_dataset = \"clean_balanced_data\"              # Your original dataset\n",
    "output_dataset = \"clean_balanced_data_balanced\"    # Output after filtering and balancing\n",
    "remove_noisy_and_balance(input_dataset, output_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65784eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Noisy Image Count Per Class:\n",
      "  CNV: 0/47528 images flagged as noisy (0.00%)\n",
      "  DME: 0/47528 images flagged as noisy (0.00%)\n",
      "  DRUSEN: 0/47528 images flagged as noisy (0.00%)\n",
      "  NORMAL: 0/47528 images flagged as noisy (0.00%)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "def is_noisy(image_path, blur_thresh=100.0, var_thresh=10.0, entropy_thresh=3.0):\n",
    "    try:\n",
    "        img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if img is None:\n",
    "            return True  # unreadable or empty image\n",
    "\n",
    "        # 1. Blur detection using Laplacian variance\n",
    "        lap_var = cv2.Laplacian(img, cv2.CV_64F).var()\n",
    "        if lap_var < blur_thresh:\n",
    "            return True\n",
    "\n",
    "        # 2. Pixel variance (flat images)\n",
    "        pixel_var = np.var(img)\n",
    "        if pixel_var < var_thresh:\n",
    "            return True\n",
    "\n",
    "        # 3. Entropy (information content)\n",
    "        histogram = cv2.calcHist([img], [0], None, [256], [0, 256]).ravel()\n",
    "        histogram /= histogram.sum()  # normalize\n",
    "        histogram = histogram[histogram > 0]  # remove zeros to avoid log(0)\n",
    "        entropy = -np.sum(histogram * np.log2(histogram))\n",
    "        if entropy < entropy_thresh:\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "    except:\n",
    "        return True  # treat unreadable as noisy\n",
    "\n",
    "def count_noisy_images(folder_path):\n",
    "    noisy_counts = defaultdict(int)\n",
    "    total_counts = defaultdict(int)\n",
    "\n",
    "    for class_dir in os.listdir(folder_path):\n",
    "        class_path = os.path.join(folder_path, class_dir)\n",
    "        if not os.path.isdir(class_path):\n",
    "            continue\n",
    "\n",
    "        for fname in os.listdir(class_path):\n",
    "            fpath = os.path.join(class_path, fname)\n",
    "            total_counts[class_dir] += 1\n",
    "\n",
    "            if is_noisy(fpath):\n",
    "                noisy_counts[class_dir] += 1\n",
    "\n",
    "    return noisy_counts, total_counts\n",
    "\n",
    "# Set your dataset path\n",
    "dataset_path = \"clean_balanced_data_balanced\"\n",
    "noisy_counts, total_counts = count_noisy_images(dataset_path)\n",
    "\n",
    "# Print the summary\n",
    "print(\"\\n📊 Noisy Image Count Per Class:\")\n",
    "for cls in total_counts:\n",
    "    noisy = noisy_counts[cls]\n",
    "    total = total_counts[cls]\n",
    "    print(f\"  {cls}: {noisy}/{total} images flagged as noisy ({(noisy/total)*100:.2f}%)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c977dc1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Scanning and filtering noisy images...\n",
      "  CNV: 50202 clean images\n",
      "  DME: 42027 clean images\n",
      "  DRUSEN: 41526 clean images\n",
      "  NORMAL: 51261 clean images\n",
      "\n",
      "🎯 Target image count per class (based on smallest class): 41526\n",
      "\n",
      "📦 Copying balanced dataset...\n",
      "  CNV: 41526 images copied\n",
      "  DME: 41526 images copied\n",
      "  DRUSEN: 41526 images copied\n",
      "  NORMAL: 41526 images copied\n",
      "\n",
      "✅ Done! Noisy images removed and classes balanced.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import shutil\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "def is_noisy(image_path, blur_thresh=100.0, var_thresh=10.0, entropy_thresh=3.0):\n",
    "    try:\n",
    "        img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if img is None:\n",
    "            return True  # unreadable or empty image\n",
    "\n",
    "        lap_var = cv2.Laplacian(img, cv2.CV_64F).var()\n",
    "        if lap_var < blur_thresh:\n",
    "            return True\n",
    "\n",
    "        pixel_var = np.var(img)\n",
    "        if pixel_var < var_thresh:\n",
    "            return True\n",
    "\n",
    "        histogram = cv2.calcHist([img], [0], None, [256], [0, 256]).ravel()\n",
    "        histogram /= histogram.sum()\n",
    "        histogram = histogram[histogram > 0]\n",
    "        entropy = -np.sum(histogram * np.log2(histogram))\n",
    "        if entropy < entropy_thresh:\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "    except:\n",
    "        return True\n",
    "\n",
    "def remove_noisy_and_balance(input_dir, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    class_images = defaultdict(list)\n",
    "\n",
    "    print(\"🔍 Scanning and filtering noisy images...\")\n",
    "    for class_name in os.listdir(input_dir):\n",
    "        class_path = os.path.join(input_dir, class_name)\n",
    "        if not os.path.isdir(class_path):\n",
    "            continue\n",
    "\n",
    "        clean_images = []\n",
    "        for img_name in os.listdir(class_path):\n",
    "            img_path = os.path.join(class_path, img_name)\n",
    "            if not is_noisy(img_path):\n",
    "                clean_images.append(img_path)\n",
    "\n",
    "        class_images[class_name] = clean_images\n",
    "        print(f\"  {class_name}: {len(clean_images)} clean images\")\n",
    "\n",
    "    # Find the minimum count\n",
    "    min_count = min(len(imgs) for imgs in class_images.values())\n",
    "    print(f\"\\n🎯 Target image count per class (based on smallest class): {min_count}\\n\")\n",
    "\n",
    "    print(\"📦 Copying balanced dataset...\")\n",
    "    for class_name, images in class_images.items():\n",
    "        selected_images = random.sample(images, min_count)\n",
    "        dest_class_dir = os.path.join(output_dir, class_name)\n",
    "        os.makedirs(dest_class_dir, exist_ok=True)\n",
    "\n",
    "        for src_path in selected_images:\n",
    "            dst_path = os.path.join(dest_class_dir, os.path.basename(src_path))\n",
    "            shutil.copy(src_path, dst_path)\n",
    "\n",
    "        print(f\"  {class_name}: {len(selected_images)} images copied\")\n",
    "\n",
    "    print(\"\\n✅ Done! Noisy images removed and classes balanced.\")\n",
    "\n",
    "# === Usage ===\n",
    "input_dataset = \"augmented_balanced_data\"              # Your original dataset\n",
    "output_dataset = \"augmented_data_balanced_clean\"    # Output after filtering and balancing\n",
    "remove_noisy_and_balance(input_dataset, output_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7e388f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: requests\n",
      "Version: 2.32.3\n",
      "Summary: Python HTTP for Humans.\n",
      "Home-page: https://requests.readthedocs.io\n",
      "Author: Kenneth Reitz\n",
      "Author-email: me@kennethreitz.org\n",
      "License: Apache-2.0\n",
      "Location: c:\\users\\dell\\anaconda3\\envs\\eyediseaseclassification_env\\lib\\site-packages\n",
      "Requires: certifi, charset-normalizer, idna, urllib3\n",
      "Required-by: requests-oauthlib, streamlit, tensorboard\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4030c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eyediseaseclassification_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
